{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of NLP_Week1_Task.ipynb","provenance":[{"file_id":"1FuCDER0PHpumDE7A7082glplaJFD4HSz","timestamp":1636658103150}]},"interpreter":{"hash":"6f6a9d06c0fd64bf60e6e1c5f52d9e61e3a0d126987fc458258eb07643fb7172"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DXG8z6NmkRDz"},"source":["# Natural Language Processing:\n","Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken and written -- referred to as natural language. It is a component of artificial intelligence (AI).\n","\n","NLP has existed for more than 50 years and has roots in the field of linguistics. It has a variety of real-world applications in a number of fields, including medical research, search engines and business intelligence.\n","\n","Natural language processing (NLP) is a form of AI that extracts meaning from human language to make decisions based on the information. This technology is still evolving, but there are already many incredible ways natural language processing is used today. Here are some of the everyday uses of natural language processing are highlighted and five amazing examples of how natural language processing is transforming businesses:\n","\n","**Examples of Natural Language Processing:**\n","\n","\n","*   NLP helps the Livox app be a communication device for people with disabilities. The creation of Carlos Pereira, a father who developed the app to help his non-verbal daughter, who has cerebral palsy communicate, the customizable app is now available in 25 languages. \n","*   Another tool enabled by natural language processing is SignAll that converts sign language into text. This can help individuals who are deaf communicate with those who don’t know sign language.\n","*   Machine translation is a huge application for NLP that allows us to overcome barriers to communicating with individuals from around the world as well as understand tech manuals and catalogs written in a foreign language. Google Translate is used by 500 million people every day to understand more than 100 world languages.\n","*    NLP technology is even being applied for aircraft maintenance. Not only could it help mechanics synthesize information from enormous aircraft manuals it can also find meaning in the descriptions of problems reported verbally or handwritten from pilots and other humans.\n","*    While the issue is complex, there’s even work being done to have NLP assist with predictive police work to specifically identify the motive in crimes.\n","* Chatbots: Another great example of language processing is chatbots. They are very much like virtual assistants but with more specific goals. They are most commonly used on websites where customers visit. They help you to get the information you need without talking to any real person. First, they try to understand your need and then bring the results in front of you.\n","* Web Scraping: Web scraping is another field where language processing is commonly used. It is used to extract information from a web page without even spending time to copy each paragraph one by one. Web scraping is a great way to collect valuable data and train your machine learning model. Web scraping is also a very helpful tool when working with search engine optimization.\n","\n","As industry leaders continue to experiment and develop enhancements to natural language processing such as Amazon’s Alexa division using a neural network to transfer learning, we can expect that NLP will become even better and more influential for business in the very near future.\n","\n","\n","\n","There are two main phases to natural language processing: data preprocessing and algorithm development.\n","\n","Data preprocessing involves preparing and \"cleaning\" text data for machines to be able to analyze it. preprocessing puts data in workable form and highlights features in the text that an algorithm can work with. There are several ways this can be done, including:\n","\n","* **Tokenization**. This is when text is broken down into smaller units to work with.\n","* **Stop word removal**. This is when common words are removed from text so unique words that offer the most information about the text remain.\n","* **Lemmatization and stemming**. This is when words are reduced to their root forms to process.\n","* **Part-of-speech tagging**. This is when words are marked based on the part-of speech they are -- such as nouns, verbs and adjectives\n","\n","Once the data has been preprocessed, an algorithm is developed to process it. There are many different natural language processing algorithms, but two main types are commonly used:\n","\n","* **Rules-based system.** This system uses carefully designed linguistic rules. This approach was used early on in the development of natural language processing, and is still used.\n","* **Machine learning-based system**. Machine learning algorithms use statistical methods. They learn to perform tasks based on training data they are fed, and adjust their methods as more data is processed. Using a combination of machine learning, deep learning and neural networks, natural language processing algorithms hone their own rules through repeated processing and learning.\n","\n","\n","# NLTK (Natural Language Toolkit):\n","NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries.\n","\n","Reference: http://www.nltk.org\n","\n","We have to install NLTK module so that we can use it in our project."]},{"cell_type":"markdown","metadata":{"id":"P-OUZIF9kREJ"},"source":["To prepare the text data for the model building we perform text preprocessing. It is the very first step of NLP projects. Some of the preprocessing steps are:\n","\n","* Removing punctuations like . , ! $( ) * % @\n","* Removing URLs\n","* Removing Stop words\n","* Lower casing\n","* Tokenization\n","* Stemming\n","* Lemmatization"]},{"cell_type":"markdown","metadata":{"id":"SGo1nmqykREJ"},"source":["### Follow this tutorial to install and setup nltk: https://www.youtube.com/watch?v=68aHmFcO-W4"]},{"cell_type":"markdown","metadata":{"id":"hKXpBEhmkREK"},"source":["### Go through this blog:<br> https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n","\n","### Understand how the pre-processing steps are applied and solve the given tasks below. Print the text after every step."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P6AWPQHWkREL","executionInfo":{"status":"ok","timestamp":1636815317154,"user_tz":-330,"elapsed":16,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"5ff49509-c232-4d7c-e3a4-524f5fc46f5e"},"source":["#Given below is a string paragraph on which we will perform the various pre processing steps\n","import re\n","import nltk\n","from IPython.core.display import HTML\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","text= \"A small sample of texts from Project Gutenberg appears in the NLTK corpus collection. However, you may be interested in analyzing other texts from Project Gutenberg. You can browse the catalog of 25,000 free online books at http://www.gutenberg.org/catalog/, and obtain a URL to an ASCII text file. Although 90% of the texts in Project Gutenberg are in English, it includes material in over 50 other languages, including Catalan, Chinese, Dutch, Finnish, French, German, Italian, Portuguese and Spanish (with more than 100 texts each). Text number 2554 is an English translation of Crime and Punishment, and we can access it as follows.\""],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"markdown","metadata":{"id":"cLIwR9HNkREN"},"source":["## Removing punctuations:\n","### Different ways to remove punctuations: https://www.pythonpool.com/remove-punctuation-python/\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l9ULzbl0kREN","executionInfo":{"status":"ok","timestamp":1636815317155,"user_tz":-330,"elapsed":10,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"d3ccb182-d71a-4c3a-9413-38c8d10f5636"},"source":["#Remove punctuations from text using any of the methods provided in the links above\n","import string \n","c_text = text.translate(str.maketrans('', '', string.punctuation))\n","print('String without Punctuation : ', c_text)"],"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["String without Punctuation :  A small sample of texts from Project Gutenberg appears in the NLTK corpus collection However you may be interested in analyzing other texts from Project Gutenberg You can browse the catalog of 25000 free online books at httpwwwgutenbergorgcatalog and obtain a URL to an ASCII text file Although 90 of the texts in Project Gutenberg are in English it includes material in over 50 other languages including Catalan Chinese Dutch Finnish French German Italian Portuguese and Spanish with more than 100 texts each Text number 2554 is an English translation of Crime and Punishment and we can access it as follows\n"]}]},{"cell_type":"markdown","metadata":{"id":"qJslFSGONFYX"},"source":["## Removing URLs:\n","\n","Search up how to remove URLs from text and apply it below"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaZvmvQ8NP3I","executionInfo":{"status":"ok","timestamp":1636815317740,"user_tz":-330,"elapsed":33,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"0947422b-52f5-48bc-ca3b-8733c60319c4"},"source":["#Write your code below to remove URL from text\n","c_text = re.sub(r\"http\\S+\", \"\", c_text)\n","print('String without URL : ', c_text)"],"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["String without URL :  A small sample of texts from Project Gutenberg appears in the NLTK corpus collection However you may be interested in analyzing other texts from Project Gutenberg You can browse the catalog of 25000 free online books at  and obtain a URL to an ASCII text file Although 90 of the texts in Project Gutenberg are in English it includes material in over 50 other languages including Catalan Chinese Dutch Finnish French German Italian Portuguese and Spanish with more than 100 texts each Text number 2554 is an English translation of Crime and Punishment and we can access it as follows\n"]}]},{"cell_type":"markdown","metadata":{"id":"JyfbfkstNvZ7"},"source":["## Removing numbers\n","\n","Search how to remove numbers from text and apply it below"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5gsU6lLN2BA","executionInfo":{"status":"ok","timestamp":1636815317744,"user_tz":-330,"elapsed":22,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"15bf115f-3b92-4620-927d-49d77560088e"},"source":["#Write your code below to remove numbers from text\n","urlless = c_text\n","pattern = r'[0-9]'\n","urlless = re.sub(pattern, '', str(urlless))\n","print(urlless)"],"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["A small sample of texts from Project Gutenberg appears in the NLTK corpus collection However you may be interested in analyzing other texts from Project Gutenberg You can browse the catalog of  free online books at  and obtain a URL to an ASCII text file Although  of the texts in Project Gutenberg are in English it includes material in over  other languages including Catalan Chinese Dutch Finnish French German Italian Portuguese and Spanish with more than  texts each Text number  is an English translation of Crime and Punishment and we can access it as follows\n"]}]},{"cell_type":"markdown","metadata":{"id":"WAu7P6ngkREO"},"source":["## Lowering the text:\n","Refer to the link mentioned above and lower the case of the text "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2fLB5btPkREP","executionInfo":{"status":"ok","timestamp":1636815317746,"user_tz":-330,"elapsed":17,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"263c2fd7-32ab-42c2-ce4c-4c60d678f834"},"source":["#Write your code below to convert text to lowercase:\n","numless = urlless\n","numless = numless.lower()\n","print(numless)"],"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["a small sample of texts from project gutenberg appears in the nltk corpus collection however you may be interested in analyzing other texts from project gutenberg you can browse the catalog of  free online books at  and obtain a url to an ascii text file although  of the texts in project gutenberg are in english it includes material in over  other languages including catalan chinese dutch finnish french german italian portuguese and spanish with more than  texts each text number  is an english translation of crime and punishment and we can access it as follows\n"]}]},{"cell_type":"markdown","metadata":{"id":"qWZEkPHhkREQ"},"source":["## Tokenisation:\n","\n"," In this step, the text is split into smaller units. We can use either sentence tokenization or word tokenization based on our problem statement.\n","\n","Learn more about tokenization here: https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/"]},{"cell_type":"code","metadata":{"id":"2zmmLqjlkREQ","executionInfo":{"status":"ok","timestamp":1636815317747,"user_tz":-330,"elapsed":12,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}}},"source":["#Perform tokenization on our text using nltk\n","from nltk.tokenize import word_tokenize\n","tokenized_text = word_tokenize(numless)"],"execution_count":52,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5nEQUpLkREQ"},"source":["## Stop word removal\n","\n","Stopwords are the commonly used words and are removed from the text as they do not add any value to the analysis. These words carry less or no meaning.\n","\n"," NLTK library consists of a list of words that are considered stopwords for the English language. Some of them are : [i, me, my, myself, we, our, ours, ourselves, you, you’re, you’ve, you’ll, you’d, your, yours, yourself, yourselves, he, most, other, some, such, no, nor, not, only, own, same, so, then, too, very, s, t, can, will, just, don, don’t, should, should’ve, now, d, ll, m, o, re, ve, y, ain, aren’t, could, couldn’t, didn’t, didn’t]\n","\n","But it is not necessary to use the provided list as stopwords as they should be chosen wisely based on the project. For example, ‘How’ can be a stop word for a model but can be important for some other problem where we are working on the queries of the customers. We can create a customized list of stop words for different problems."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mW4oea2fkRER","executionInfo":{"status":"ok","timestamp":1636815318118,"user_tz":-330,"elapsed":17,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"d3907243-45c8-43fa-a731-50e39a9f4bb9"},"source":["#Remove stop words from text using NLTK\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","removed_stop_words = []\n","for word in tokenized_text:\n","  if word not in stop_words:\n","    removed_stop_words.append(word)\n","print(removed_stop_words)"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["['small', 'sample', 'texts', 'project', 'gutenberg', 'appears', 'nltk', 'corpus', 'collection', 'however', 'may', 'interested', 'analyzing', 'texts', 'project', 'gutenberg', 'browse', 'catalog', 'free', 'online', 'books', 'obtain', 'url', 'ascii', 'text', 'file', 'although', 'texts', 'project', 'gutenberg', 'english', 'includes', 'material', 'languages', 'including', 'catalan', 'chinese', 'dutch', 'finnish', 'french', 'german', 'italian', 'portuguese', 'spanish', 'texts', 'text', 'number', 'english', 'translation', 'crime', 'punishment', 'access', 'follows']\n"]}]},{"cell_type":"markdown","metadata":{"id":"77ZcUcXOkRER"},"source":["## Stemming:<br>\n","Stemming and lemmatization are really important. Read about them here: <br>https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n","<br> Watch some videos on YouTube if you want a deeper understanding!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"F5hIqgVnkRES","executionInfo":{"status":"ok","timestamp":1636815319627,"user_tz":-330,"elapsed":18,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"fd6401da-b469-457a-df6e-089f5b2df479"},"source":["#Apply stemming(PorterStemmer) to the text we got after stopword removal and store the resulting list in a\n","# variable called \"stemmed_text\"\n","# Write code here\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","ps = PorterStemmer()\n","stemmed_text = []\n","\n","for word in removed_stop_words:\n","  stemmed_text.append(ps.stem(word))\n","\n","# DO NOT CHANGE\n","assert stemmed_text == ['small','sampl','text','project','gutenberg','appear','nltk','corpu','collect','howev','may','interest','analyz','text','project','gutenberg','brows','catalog','free','onlin','book','obtain','url','ascii','text','file','although','text','project','gutenberg','english','includ','materi','languag','includ','catalan','chines','dutch','finnish','french','german','italian','portugues','spanish','text','text','number','english','translat','crime','punish','access','follow'], \\\n","       \"Hmmm...your list doesn't seem to match with ours.\"\n","HTML('<div class=\"alert alert-block alert-success\">Your list look good!</div>')"],"execution_count":54,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div class=\"alert alert-block alert-success\">Your list look good!</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","metadata":{"id":"iYJxJRxJkRES"},"source":["## Lemmatization:"]},{"cell_type":"code","metadata":{"id":"0__C4BLOkRES","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1636815323191,"user_tz":-330,"elapsed":411,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"f2ab96ca-bba4-4112-8f3e-41cfe022e7b7"},"source":["#Apply lemmatization (WordNetLemmatizer) to the text we got after stopword removal, and store the resulting list in a\n","# variable called \"lemmatized_text\":\n","# Write code here\n","from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_text = []\n","\n","for t in removed_stop_words:\n","    lemmatized_text.append(lemmatizer.lemmatize(t))\n","    \n","print (lemmatized_text)\n","\n","# DO NOT CHANGE\n","assert lemmatized_text == ['small', 'sample', 'text', 'project', 'gutenberg', 'appears', 'nltk', 'corpus', 'collection', 'however', 'may', 'interested', 'analyzing', 'text', 'project', 'gutenberg', 'browse', 'catalog', 'free', 'online', 'book', 'obtain', 'url', 'ascii', 'text', 'file', 'although', 'text', 'project', 'gutenberg', 'english', 'includes', 'material', 'language', 'including', 'catalan', 'chinese', 'dutch', 'finnish', 'french', 'german', 'italian', 'portuguese', 'spanish', 'text', 'text', 'number', 'english', 'translation', 'crime', 'punishment', 'access', 'follows'], \\\n","       \"Hmmm...your list doesn't seem to match with ours.\"\n","HTML('<div class=\"alert alert-block alert-success\">Your list look good!</div>')"],"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["['small', 'sample', 'text', 'project', 'gutenberg', 'appears', 'nltk', 'corpus', 'collection', 'however', 'may', 'interested', 'analyzing', 'text', 'project', 'gutenberg', 'browse', 'catalog', 'free', 'online', 'book', 'obtain', 'url', 'ascii', 'text', 'file', 'although', 'text', 'project', 'gutenberg', 'english', 'includes', 'material', 'language', 'including', 'catalan', 'chinese', 'dutch', 'finnish', 'french', 'german', 'italian', 'portuguese', 'spanish', 'text', 'text', 'number', 'english', 'translation', 'crime', 'punishment', 'access', 'follows']\n"]},{"output_type":"execute_result","data":{"text/html":["<div class=\"alert alert-block alert-success\">Your list look good!</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"o1ncmAyTOPsS"},"source":["### Now that the text is cleaned, the next step is to transform it into a form that can be fed into a machine learning model. We'll dive deeper into this next week, but let's go through one technique and see what word embeddings are!"]},{"cell_type":"markdown","metadata":{"id":"OZxw0GvlOPsT"},"source":["# An Introduction to Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"L_bfwvcQOPsT"},"source":["Many people would say the breakthrough of deep learning in Natural Language Processing started with the introduction of word embeddings. Rather than using the words themselves as features, neural network methods typically take as input dense, relatively low-dimensional vectors that model the meaning and usage of a word. Word embeddings were first popularized through the [Word2Vec](https://arxiv.org/abs/1301.3781) model, developed by Thomas Mikolov and colleagues at Google. Since then, scores of alternative approaches have been developed, such as [GloVe](https://nlp.stanford.edu/projects/glove/) and [FastText](https://fasttext.cc/) embeddings. In this notebook, we'll explore word embeddings with the original Word2Vec approach, as implemented in the [Gensim](https://radimrehurek.com/gensim/) library. "]},{"cell_type":"markdown","metadata":{"id":"veebmbXFOPsU"},"source":["## Training word embeddings"]},{"cell_type":"markdown","metadata":{"id":"9t5RgxGkOPsU"},"source":["Training word embeddings with Gensim couldn't be easier. The only thing we need is a corpus of sentences in the language under investigation. For our experiments, we're going to use the abstracts of all ArXiv papers in the category cs.CL (computation and language) that were published before mid-April 2021 — a total of around 25,000 documents. We tokenize these abstracts with spaCy. "]},{"cell_type":"code","metadata":{"id":"eVCDV0nxOPsU","executionInfo":{"status":"ok","timestamp":1636815417379,"user_tz":-330,"elapsed":429,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}}},"source":["import os\n","import csv\n","import spacy\n","\n","class Corpus(object):\n","\n","    def __init__(self, filename):\n","        self.filename = filename\n","        self.nlp = spacy.blank(\"en\")\n","        \n","    def __iter__(self):\n","        with open(self.filename, \"r\") as i:\n","            reader = csv.reader(i, delimiter=\",\")\n","            for _, abstract in reader:\n","                tokens = [t.text.lower() for t in self.nlp(abstract)]\n","                yield tokens\n","                            \n","                    \n","documents = Corpus(\"./arxiv.csv\")"],"execution_count":60,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S9ZEN6TQOPsV"},"source":["When we train our word embeddings, gensim allows us to set a number of parameters. The most important of these are `min_count`, `window`, `vector_size` and `sg`:\n","\n","- `min_count` is the minimum frequency of the words in our corpus. For infrequent words, we just don't have enough information to train reliable word embeddings. It therefore makes sense to set this minimum frequency to at least 10. In these experiments, we'll set it to 100 to limit the size of our model even more.\n","- `window` is the number of words to the left and to the right that make up the context that word2vec will take into account.\n","- `vector_size` is the dimensionality of the word vectors. This is generally between 100 and 1000. This dimensionality often forces us to make a trade-off: embeddings with a higher dimensionality are able to model more information, but also need more data to train.\n","- `sg`: there are two algorithms to train word2vec: skip-gram and CBOW. Skip-gram tries to predict the context on the basis of the target word; CBOW tries to find the target on the basis of the context. By default, Gensim uses CBOW (`sg=0`).\n","\n","We'll investigate the impact of some of these parameters later."]},{"cell_type":"code","metadata":{"id":"1VRhWWBROPsV","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"error","timestamp":1636815569710,"user_tz":-330,"elapsed":1382,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"56c60263-a8e8-483b-cf42-35012e0d9baa"},"source":["import gensim\n","# will take some time to run\n","model = gensim.models.Word2Vec(documents, min_count=100, window=5, size=100)"],"execution_count":62,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-fdd8b00a04a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# will take some time to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     def _do_train_epoch(self, corpus_file, thread_id, offset, cython_vocab, thread_private_mem, cur_epoch,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You can't pass a generator as the sentences argument. Try an iterator.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m             self.train(\n\u001b[1;32m    761\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \"\"\"\n\u001b[1;32m    935\u001b[0m         total_words, corpus_count = self.vocabulary.scan_vocab(\n\u001b[0;32m--> 936\u001b[0;31m             sentences=sentences, corpus_file=corpus_file, progress_per=progress_per, trim_rule=trim_rule)\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, sentences, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m   1569\u001b[0m             \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m         logger.info(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m   1538\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0mchecked_string_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msentence_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchecked_string_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-60-9a9162efeb4c>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'arxiv.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"NSc2D5mVOPsV"},"source":["## Using word embeddings"]},{"cell_type":"markdown","metadata":{"id":"_LH3C4uuOPsV"},"source":["Let's take a look at the trained model. The word embeddings are on its `wv` attribute, and we can access them by the using the token as key. For example, here is the embedding for *nlp*, with the requested 100 dimensions."]},{"cell_type":"code","metadata":{"id":"B7BXkRaZOPsW","colab":{"base_uri":"https://localhost:8080/","height":162},"executionInfo":{"status":"error","timestamp":1636814365222,"user_tz":-330,"elapsed":18,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"e93d7598-d3ee-45e3-9b24-f09f6f12c081"},"source":["model.wv[\"nlp\"]"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-b75600cb1565>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nlp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"6bxgDIiuOPsW"},"source":["We can also easily find the similarity between two words. Similarity is measured as the cosine between the two word embeddings, and therefore ranges between -1 and +1. The higher the cosine, the more similar two words are. As expected, the figures below show that *nmt* (neural machine translation) is closer to *smt* (statistical machine translation) than to *ner* (named entity recognition)."]},{"cell_type":"code","metadata":{"id":"ZH2K9LcZOPsW","colab":{"base_uri":"https://localhost:8080/","height":178},"executionInfo":{"status":"error","timestamp":1636814366599,"user_tz":-330,"elapsed":21,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"a34248e5-502d-47dc-bb40-67f8f5e49e92"},"source":["print(model.wv.similarity(\"nmt\", \"smt\"))\n","print(model.wv.similarity(\"nmt\", \"ner\"))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-2b91833f4a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nmt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"smt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nmt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"og7zrG5wOPsX"},"source":["In a similar vein, we can find the words that are most similar to a target word. The words with the most similar embedding to *bert* are all semantically related to it: other types of pretrained models such as *roberta*, *mbert*, *xlm*, as well as the more general model type BERT represents (*transformer* and *transformers*), and more generally related words (*pretrained*)."]},{"cell_type":"code","metadata":{"id":"JEzdLhouOPsX"},"source":["model.wv.similar_by_word(\"bert\", topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fO1EQylJOPsX"},"source":["Interestingly, we can look for words that are similar to a set of words and dissimilar to another set of words at the same time. This allows us to look for analogies of the type *BERT is to a transformer like an LSTM is to ...*. Our embedding model correctly predicts that LSTMs are a type of RNN, just like BERT is a particular type of transformer."]},{"cell_type":"code","metadata":{"id":"-oVN8W-POPsX"},"source":["model.wv.most_similar(positive=[\"transformer\", \"lstm\"], negative=[\"bert\"], topn=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mImLnLw9OPsY"},"source":["Similarly, we can also zoom in on one of the meanings of ambiguous words. For example, in NLP *tree* has a very specific meaning, which is obvious from its nearest neighbours *constituency*, *parse*, *dependency* and *syntax*. "]},{"cell_type":"code","metadata":{"id":"vkxRUrdUOPsY"},"source":["model.wv.most_similar(positive=[\"tree\"], topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dg8G0qV6OPsY"},"source":["However, if we specify we're looking for words that are similar to *tree*, but dissimilar to *syntax*, suddenly its standard meaning takes over, and *forest* crops up in its nearest neighbours."]},{"cell_type":"code","metadata":{"id":"WsidBM2AOPsZ"},"source":["model.wv.most_similar(positive=[\"tree\"], negative=[\"syntax\"], topn=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rEptwgBDOPsZ"},"source":["Finally, we can present the word2vec model with a list of words and ask it to identify the odd one out. It then uses the word embeddings to identify the word that is least similar to the other ones. For example, in the list *lstm cnn gru svm transformer*, it correctly identifies *svm* as the only non-neural model. In the list *bert word2vec gpt-2 roberta xlnet*, it correctly singles out *word2vec* as the only non-transormer model. In *word2vec bert glove fasttext elmo*, *bert* is singled out as the only transformer."]},{"cell_type":"code","metadata":{"id":"C0hSyzx7OPsZ"},"source":["print(model.wv.doesnt_match(\"lstm cnn gru svm transformer\".split()))\n","print(model.wv.doesnt_match(\"bert word2vec gpt-2 roberta xlnet\".split()))\n","print(model.wv.doesnt_match(\"word2vec bert glove fasttext elmo\".split()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ahuyUFkOPsa"},"source":["## Plotting embeddings"]},{"cell_type":"markdown","metadata":{"id":"oN3QMr9aOPsa"},"source":["Let's now visualize some of our embeddings. To plot embeddings with a dimensionality of 100 or more, we first need to map them to a dimensionality of 2. We do this with the popular [t-SNE](https://lvdmaaten.github.io/tsne/) method. T-SNE, short for t-distributed Stochastic Neighbor Embedding, helps us visualize high-dimensional data by mapping similar data to nearby points and dissimilar data to distance points in the low-dimensional space.\n","\n","T-SNE is present in [Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). To run it, we just have to specify the number of dimensions we'd like to map the data to (`n_components`), and the similarity metric that t-SNE should use to compute the similarity between two data points (`metric`). We're going to map to 2 dimensions and use the cosine as our similarity metric. Additionally, we use PCA as an initialization method to remove some noise and speed up computation. The [Scikit-learn user guide](https://scikit-learn.org/stable/modules/manifold.html#t-sne) contains some additional tips for optimizing performance. \n","\n","Plotting all the embeddings in our vector space would result in a very crowded figure where the labels are hardly legible. Therefore we'll focus on a subset of embeddings by selecting the 200 most similar words to a target word. "]},{"cell_type":"code","metadata":{"id":"bKfaqkWoOPsb"},"source":["%matplotlib inline\n","\n","import pandas as pd\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","from sklearn.manifold import TSNE\n","\n","target_word = \"bert\"\n","selected_words = [w[0] for w in model.wv.most_similar(positive=[target_word], topn=200)] + [target_word]\n","embeddings = [model.wv[w] for w in selected_words] + model.wv[\"bert\"]\n","\n","mapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BEHahL5iOPsb"},"source":["If we take *bert* as our target word, the figure shows some interesting patterns. In the immediate vicinity of *bert*, we find the similar transformer models that we already identified as its nearest neighbours earlier: *xlm*, *mbert*, *gpt-2*, and so on. Other parts of the picture have equally informative clusters of NLP tasks and benchmarks (*squad* and *glue*), languages (*german* and *english*), neural-network architectures (*lstm*, *gru*, etc.), embedding types (*word2vec*, *glove*, *fasttext*, *elmo*), etc. "]},{"cell_type":"code","metadata":{"id":"4oFyYkEFOPsb"},"source":["plt.figure(figsize=(20,20))\n","x = mapped_embeddings[:,0]\n","y = mapped_embeddings[:,1]\n","plt.scatter(x, y)\n","\n","for i, txt in enumerate(selected_words):\n","    plt.annotate(txt, (x[i], y[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TFS5GyzoOPsb"},"source":["## Conclusions"]},{"cell_type":"markdown","metadata":{"id":"uLjfPB0nOPsc"},"source":["Word embeddings are one of the most exciting trends on Natural Language Processing since the 2000s. They allow us to model the meaning and usage of a word, and discover words that behave similarly. This is crucial for the generalization capacity of many machine learning models. Moving from raw strings to embeddings allows them to generalize across words that have a similar meaning, and discover patterns that had previously escaped them."]}]}