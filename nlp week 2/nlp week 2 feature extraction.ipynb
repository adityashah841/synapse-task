{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"Copy of Copy of Feature_Extraction.ipynb","provenance":[{"file_id":"1Dn2qBbFpbAxsMmaDqAX5APiiUKM3Z3_W","timestamp":1637434449536},{"file_id":"1FBVCePfG4eqMeUL4OqOz__dGbfG6EZnH","timestamp":1637307641082}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"As7a2I_6f26W"},"source":["<a id=\"Introduction\"></a>\n","# Introduction\n","\n","This week we will be focusing on NLP preprocessing and feature extraction techniques and codes. We will mainly use the [Real or Not? NLP with Disaster Tweets](https://www.kaggle.com/c/nlp-getting-started) dataset for illustration.\n","\n","We will revise our concepts learned from last week and will be learning newer concepts based on feature extraction and word embeddings\n","\n","<img src=\"https://miro.medium.com/max/1750/1*rJQVqDjbhI3k22lHqa4dFw.png\" align=\"center\"/>\n","\n","image source: [Natural Language Processing Pipeline](https://towardsdatascience.com/natural-language-processing-pipeline-93df02ecd03f)"]},{"cell_type":"markdown","metadata":{"id":"GEAmnyQzf26g"},"source":["<a id=\"Read_and_explore_data\"></a>\n","\n","# Read and explore data\n","\n","<a id=\"Importing_Main_Packages\"></a>\n","## Importing Main Packages"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:32.194579Z","iopub.status.busy":"2021-11-13T10:20:32.194211Z","iopub.status.idle":"2021-11-13T10:20:32.234873Z","shell.execute_reply":"2021-11-13T10:20:32.234227Z","shell.execute_reply.started":"2021-11-13T10:20:32.194548Z"},"scrolled":true,"id":"Kl8zv1uYf26i","executionInfo":{"status":"ok","timestamp":1637605297860,"user_tz":-330,"elapsed":14,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}}},"source":["\n","import sys\n","import numpy as np\n","import pandas as pd\n","import sklearn\n","\n","# Libraries and packages for text (pre-)processing \n","import string\n","import re\n","import nltk\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U675L9Cbf26l"},"source":["<a id=\"Read_the_Data\"></a>\n","## Read the Data"]},{"cell_type":"code","metadata":{"id":"fG4JQgghlwrw","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1637605331215,"user_tz":-330,"elapsed":33367,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"e8b12a0d-4090-4f95-bbc2-750ea5727dcc"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-fd0617c0-90a3-4cc1-942d-2c06399eca84\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-fd0617c0-90a3-4cc1-942d-2c06399eca84\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving train.csv to train.csv\n"]}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:32.236818Z","iopub.status.busy":"2021-11-13T10:20:32.236328Z","iopub.status.idle":"2021-11-13T10:20:32.272601Z","shell.execute_reply":"2021-11-13T10:20:32.271053Z","shell.execute_reply.started":"2021-11-13T10:20:32.236782Z"},"id":"tPM8PBhNf26n","executionInfo":{"status":"ok","timestamp":1637605331217,"user_tz":-330,"elapsed":84,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}}},"source":["# read the \"train.csv\" file and display using head(). Name the variable as \"train_df\"\n","train_df = pd.read_csv(\"train.csv\")"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:32.274127Z","iopub.status.busy":"2021-11-13T10:20:32.273855Z","iopub.status.idle":"2021-11-13T10:20:32.296992Z","shell.execute_reply":"2021-11-13T10:20:32.295936Z","shell.execute_reply.started":"2021-11-13T10:20:32.274099Z"},"scrolled":true,"id":"4i3ogyB6f26o","colab":{"base_uri":"https://localhost:8080/","height":243},"executionInfo":{"status":"ok","timestamp":1637605331218,"user_tz":-330,"elapsed":82,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"646a323c-6148-4dae-c7c9-47c5e77c1ae3"},"source":["# some data exploration\n","display(train_df[~train_df[\"location\"].isnull()].head())\n","display(train_df[train_df[\"target\"] == 0][\"text\"].values[1])\n","display(train_df[train_df[\"target\"] == 1][\"text\"].values[1])"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>31</th>\n","      <td>48</td>\n","      <td>ablaze</td>\n","      <td>Birmingham</td>\n","      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>49</td>\n","      <td>ablaze</td>\n","      <td>Est. September 2012 - Bristol</td>\n","      <td>We always try to bring the heavy. #metal #RT h...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>50</td>\n","      <td>ablaze</td>\n","      <td>AFRICA</td>\n","      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>52</td>\n","      <td>ablaze</td>\n","      <td>Philadelphia, PA</td>\n","      <td>Crying out for more! Set me ablaze</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>53</td>\n","      <td>ablaze</td>\n","      <td>London, UK</td>\n","      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    id keyword  ...                                               text target\n","31  48  ablaze  ...  @bbcmtd Wholesale Markets ablaze http://t.co/l...      1\n","32  49  ablaze  ...  We always try to bring the heavy. #metal #RT h...      0\n","33  50  ablaze  ...  #AFRICANBAZE: Breaking news:Nigeria flag set a...      1\n","34  52  ablaze  ...                 Crying out for more! Set me ablaze      0\n","35  53  ablaze  ...  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...      0\n","\n","[5 rows x 5 columns]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'I love fruits'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Forest fire near La Ronge Sask. Canada'"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"kMjjixoIf26r"},"source":["<a id=\"Text_Preprocessing\"></a>\n","\n","# Text Preprocessing:\n","\n","<a id=\"Capitalization\"></a>\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:32.299060Z","iopub.status.busy":"2021-11-13T10:20:32.298617Z","iopub.status.idle":"2021-11-13T10:20:32.319369Z","shell.execute_reply":"2021-11-13T10:20:32.318525Z","shell.execute_reply.started":"2021-11-13T10:20:32.299030Z"},"scrolled":true,"id":"Sa8jxsIrf26t","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1637605331219,"user_tz":-330,"elapsed":74,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"37990ff5-8963-4f8d-a189-63ca9f67112c"},"source":["#Write the code to remove all text capitalization and save it in a column called \"text_clean\"\n","# Complete the code below\n","# Hint: Use pandas apply()\n","cleaned = []\n","for text in train_df['text']:\n","  cleaned.append(text.lower())\n","train_df['text_clean'] = cleaned\n","train_df.head()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this #earthquake m...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask. canada</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to 'shelter in place' are ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby #alaska as ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id keyword  ... target                                         text_clean\n","0   1     NaN  ...      1  our deeds are the reason of this #earthquake m...\n","1   4     NaN  ...      1             forest fire near la ronge sask. canada\n","2   5     NaN  ...      1  all residents asked to 'shelter in place' are ...\n","3   6     NaN  ...      1  13,000 people receive #wildfires evacuation or...\n","4   7     NaN  ...      1  just got sent this photo from ruby #alaska as ...\n","\n","[5 rows x 6 columns]"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"upWfRpyJf26u"},"source":["## Noise Removal \n","Text data could include various unnecessary characters or punctuation such as URLs, HTML tags, non-ASCII characters, or other special characters (symbols, emojis, and other graphic characters). \n","\n","### Remove URLs"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:39.762075Z","iopub.status.busy":"2021-11-13T10:20:39.761848Z","iopub.status.idle":"2021-11-13T10:20:39.766745Z","shell.execute_reply":"2021-11-13T10:20:39.765345Z","shell.execute_reply.started":"2021-11-13T10:20:39.762051Z"},"scrolled":true,"id":"G_k75cxXf26w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605331221,"user_tz":-330,"elapsed":75,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"52e94783-f891-4fca-d3e1-c30de47d2d13"},"source":["#complete the function below to remove urls\n","\n","def remove_URL(sample):\n","#    \"\"\"Remove URLs from a sample string\"\"\"\n","    return re.sub(r'http\\S+', '', sample)\n","\n","# removing urls from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_URL(x))\n","\n","# double check\n","print(train_df[\"text\"][31])\n","print(train_df[\"text_clean\"][31])\n","print(train_df[\"text\"][37])\n","print(train_df[\"text_clean\"][37])\n","print(train_df[\"text\"][62])\n","print(train_df[\"text_clean\"][62])"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\n","@bbcmtd wholesale markets ablaze \n","INEC Office in Abia Set Ablaze - http://t.co/3ImaomknnA\n","inec office in abia set ablaze - \n","Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n","rene ablaze &amp; jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n"]}]},{"cell_type":"markdown","metadata":{"id":"G5DLt_NEf26y"},"source":["<a id=\"Remove_HTML_tags\"></a>\n","\n","### Remove HTML tags"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:39.833807Z","iopub.status.busy":"2021-11-13T10:20:39.833453Z","iopub.status.idle":"2021-11-13T10:20:39.839093Z","shell.execute_reply":"2021-11-13T10:20:39.837859Z","shell.execute_reply.started":"2021-11-13T10:20:39.833777Z"},"scrolled":true,"id":"gASZ-kLmf26z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605331227,"user_tz":-330,"elapsed":74,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"40d4f4e7-605f-4db7-b320-2cf0d71fdc01"},"source":["#complete the function below to remove html tags\n","\n","\n","def remove_html(text):\n","    p = re.compile(r'<.*?>')\n","    return p.sub('', str(text))\n","\n","# remove html from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_html(x))\n","\n","\n","# double check\n","print(train_df[\"text\"][62])\n","print(train_df[\"text_clean\"][62])\n","print(train_df[\"text\"][7385])\n","print(train_df[\"text_clean\"][7385])"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Rene Ablaze &amp; Jacinta - Secret 2k13 (Fallen Skies Edit) - Mar 30 2013  https://t.co/7MLMsUzV1Z\n","rene ablaze &amp; jacinta - secret 2k13 (fallen skies edit) - mar 30 2013  \n","NW Michigan #WindStorm (Sheer) Recovery Updates: Leelanau &amp; Grand Traverse - State of Emergency 2b extended http://t.co/OSKfyj8CK7 #BeSafe\n","nw michigan #windstorm (sheer) recovery updates: leelanau &amp; grand traverse - state of emergency 2b extended  #besafe\n"]}]},{"cell_type":"markdown","metadata":{"id":"WhKpQyimf260"},"source":["<a id=\"Remove_Non_ASCII\"></a>\n","\n","### Remove Non-ASCI:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:39.909211Z","iopub.status.busy":"2021-11-13T10:20:39.908999Z","iopub.status.idle":"2021-11-13T10:20:39.913459Z","shell.execute_reply":"2021-11-13T10:20:39.912481Z","shell.execute_reply.started":"2021-11-13T10:20:39.909188Z"},"scrolled":true,"id":"Fwa2DwbXf261","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605331229,"user_tz":-330,"elapsed":69,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"15308d21-fcac-4e17-faca-72c154fe07d2"},"source":["#complete the function below to remove non-ascii characters\n","\n","def remove_non_ascii(text):\n","  return re.sub(r'[^\\x00-\\x7f]', '', str(text))\n","\n","# removing non-ascii characters from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_non_ascii(x))\n","\n","# double check\n","print(train_df[\"text\"][38])\n","print(train_df[\"text_clean\"][38])\n","print(train_df[\"text\"][7586])\n","print(train_df[\"text_clean\"][7586])"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Barbados #Bridgetown JAMAICA ÛÒ Two cars set ablaze: SANTA CRUZ ÛÓ Head of the St Elizabeth Police Superintende...  http://t.co/wDUEaj8Q4J\n","barbados #bridgetown jamaica  two cars set ablaze: santa cruz  head of the st elizabeth police superintende...  \n","#Sismo DETECTADO #JapÌ_n 15:41:07 Seismic intensity 0 Iwate Miyagi JST #?? http://t.co/gMoUl9zQ2Q\n","#sismo detectado #jap_n 15:41:07 seismic intensity 0 iwate miyagi jst #?? \n"]}]},{"cell_type":"markdown","metadata":{"id":"YSAYVp-ff262"},"source":["<a id=\"Remove_punctuations\"></a>\n","\n","## Remove punctuations:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:39.953064Z","iopub.status.busy":"2021-11-13T10:20:39.952769Z","iopub.status.idle":"2021-11-13T10:20:39.958153Z","shell.execute_reply":"2021-11-13T10:20:39.957085Z","shell.execute_reply.started":"2021-11-13T10:20:39.953040Z"},"scrolled":true,"id":"CSv-zUIEf263","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605331229,"user_tz":-330,"elapsed":64,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"07b93b71-0d91-40f2-b459-a009a78d6b52"},"source":["#complete the function below to remove punctuations\n","\n","import string\n","string.punctuation\n","\n","def remove_punctuation(text):\n","    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n","    return punctuationfree\n","\n","# removing punctuations from the text\n","train_df[\"text_clean\"] = train_df[\"text_clean\"].apply(lambda x: remove_punctuation(x))\n","\n","# double check\n","print(train_df[\"text\"][5])\n","print(train_df[\"text_clean\"][5])\n","print(train_df[\"text\"][7597])\n","print(train_df[\"text_clean\"][7597])"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n","rockyfire update  california hwy 20 closed in both directions due to lake county fire  cafire wildfires\n","#??? #?? #??? #??? MH370: Aircraft debris found on La Reunion is from missing Malaysia Airlines ... http://t.co/5B7qT2YxdA\n","    mh370 aircraft debris found on la reunion is from missing malaysia airlines  \n"]}]},{"cell_type":"markdown","metadata":{"id":"m7nIbzXSf264"},"source":["<a id=\"Text_Preprocessing\"></a>\n","\n","# Text Preprocessing:\n","\n","<a id=\"Tokenization\"></a>\n","## Tokenization\n","Tokenization is a common technique that split a sentence into tokens, where a token could be characters, words, phrases, symbols, or other meaningful elements. By breaking sentences into smaller chunks, that would help to investigate the words in a sentence and also the subsequent steps in the NLP pipeline, such as stemming. "]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:40.160184Z","iopub.status.busy":"2021-11-13T10:20:40.159958Z","iopub.status.idle":"2021-11-13T10:20:41.111234Z","shell.execute_reply":"2021-11-13T10:20:41.109647Z","shell.execute_reply.started":"2021-11-13T10:20:40.160161Z"},"scrolled":true,"id":"3PPuVLivf265","colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"status":"ok","timestamp":1637605332341,"user_tz":-330,"elapsed":1171,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"fb506a4b-ba5a-49e0-d325-80043486e4fe"},"source":["# Tokenizing the tweet base texts and save it in a column called \"tokenized\"\n","import nltk\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","\n","train_df['tokenized'] = train_df['text_clean'].apply(word_tokenize) \n","\n","train_df.head()"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this earthquake ma...</td>\n","      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to shelter in place are be...</td>\n","      <td>[all, residents, asked, to, shelter, in, place...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13000 people receive wildfires evacuation orde...</td>\n","      <td>[13000, people, receive, wildfires, evacuation...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby alaska as s...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                          tokenized\n","0   1  ...  [our, deeds, are, the, reason, of, this, earth...\n","1   4  ...      [forest, fire, near, la, ronge, sask, canada]\n","2   5  ...  [all, residents, asked, to, shelter, in, place...\n","3   6  ...  [13000, people, receive, wildfires, evacuation...\n","4   7  ...  [just, got, sent, this, photo, from, ruby, ala...\n","\n","[5 rows x 7 columns]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"T4vwDYiaf266"},"source":["<a id=\"Remove_Stop_Words\"></a>\n","\n","## Remove Stop Words (or/and Frequent words/ Rare words):\n","Stop words are common words in any language that occur with a high frequency but do not deliver meaningful information for the whole sentence. For example, {“a”, “about”, “above”, “across”, “after”, “afterward”, “again”, ...} can be considered as stop words. Traditionally, we could remove all of them in the text preprocessing stage. However, refer to the example from the [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action) book: \n","> * Mark reported to the CEO\n","> * Suzanne reported as the CEO to the board \n","\n","> In your NLP pipeline, you might create 4-grams such as reported to the CEO and reported as the CEO. If you remove the stop words from the 4-grams, both examples would be reduced to \"reported CEO\", and you would lack the information about the professional hierarchy. In the first example, Mark could have been an assistant to the CEO, whereas in the second example Suzanne was the CEO reporting to the board. Unfortunately, retaining the stop words within your pipeline creates another problem: it increases the length of the n-grams required to make use of these connections formed by the otherwise meaningless stop words. This issue forces us to retain at least 4-grams if you want to avoid the ambiguity of the human resources example.\n","> Designing a filter for stop words depends on your particular application.\n","\n","In short, removing stop words is a common method in NLP text preprocessing, whereas, it needs to be experimented carefully depending on different situations. \n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:41.113323Z","iopub.status.busy":"2021-11-13T10:20:41.113005Z","iopub.status.idle":"2021-11-13T10:20:41.155762Z","shell.execute_reply":"2021-11-13T10:20:41.155096Z","shell.execute_reply.started":"2021-11-13T10:20:41.113297Z"},"id":"sZECLKvlf267","colab":{"base_uri":"https://localhost:8080/","height":310},"executionInfo":{"status":"ok","timestamp":1637605347003,"user_tz":-330,"elapsed":525,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"ea200402-0e9b-437d-c8ac-9a6d301b0d47"},"source":["# Remove stopwords from train_df['tokenized'] and save it in another column called 'stopwords_removed'\n","# Hint: Use pandas apply function with lambda function (the way we did in previous cells) and you can use list\n","# comprehension to remove stop words\n","\n","nltk.download(\"stopwords\")\n","from nltk.corpus import stopwords\n","\n","stop = set(stopwords.words('english'))\n","\n","#complete code below\n","train_df['stopwords_removed'] = train_df['tokenized'].apply(lambda x: [word for word in x if word not in stop])\n","\n","train_df.head()"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","      <th>stopwords_removed</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this earthquake ma...</td>\n","      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n","      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to shelter in place are be...</td>\n","      <td>[all, residents, asked, to, shelter, in, place...</td>\n","      <td>[residents, asked, shelter, place, notified, o...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13000 people receive wildfires evacuation orde...</td>\n","      <td>[13000, people, receive, wildfires, evacuation...</td>\n","      <td>[13000, people, receive, wildfires, evacuation...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby alaska as s...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n","      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                  stopwords_removed\n","0   1  ...  [deeds, reason, earthquake, may, allah, forgiv...\n","1   4  ...      [forest, fire, near, la, ronge, sask, canada]\n","2   5  ...  [residents, asked, shelter, place, notified, o...\n","3   6  ...  [13000, people, receive, wildfires, evacuation...\n","4   7  ...  [got, sent, photo, ruby, alaska, smoke, wildfi...\n","\n","[5 rows x 8 columns]"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"Asl7lF3Uf268"},"source":["<a id=\"Stemming\"></a>\n","\n","## Stemming\n","Stemming is a process of extracting a root word - identifying a common stem among various forms (e.g., singular and plural noun form) of a word, for example, the words \"gardening\", \"gardener\" or \"gardens\" share the same stem, garden. Stemming uproots suffixes from words to merge words with similar meanings under their standard stem.\n","\n","There are three major stemming algorithms in use nowadays:\n","- **Porter** - PorterStemmer()): This stemming algorithm is an older one. It’s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form. It’s not too complex and development on it is frozen. Typically, it’s a nice starting basic stemmer, but it’s not really advised to use it for any production/complex application. Instead, it has its place in research as a nice, basic stemming algorithm that can guarantee reproducibility. It also is a very gentle stemming algorithm when compared to others.\n","\n","- **Snowball** -  SnowballStemmer(): This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer. That being said, it is also more aggressive than the Porter stemmer. A lot of the things added to the Snowball stemmer were because of issues noticed with the Porter stemmer. There is about a 5% difference in the way that Snowball stems versus Porter.\n","\n","- **Lancaster** -LancasterStemmer(): Just for fun, the Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch. However, if you use the stemmer in NLTK, you can add your own custom rules to this algorithm very easily. It’s a good choice for that. One complaint around this stemming algorithm though is that it sometimes is overly aggressive and can really transform words into strange stems. Just make sure it does what you want it to before you go with this option!\n","\n","source: http://hunterheidenreich.com/blog/stemming-lemmatization-what/\n","\n","### PorterStemmer"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:41.157015Z","iopub.status.busy":"2021-11-13T10:20:41.156792Z","iopub.status.idle":"2021-11-13T10:20:41.162619Z","shell.execute_reply":"2021-11-13T10:20:41.161640Z","shell.execute_reply.started":"2021-11-13T10:20:41.156991Z"},"scrolled":true,"id":"B2NgNdcQf269","colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"status":"ok","timestamp":1637605377241,"user_tz":-330,"elapsed":2010,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"02a3cc5b-2d7c-4c0b-efe1-a5bdd6e4faec"},"source":["from nltk.stem import PorterStemmer\n","\n","def porter_stemmer(text):\n","    \"\"\"\n","        Stem words in list of tokenized words with PorterStemmer\n","    \"\"\"\n","    stemmer = nltk.PorterStemmer()\n","    stems = [stemmer.stem(i) for i in text]\n","    return stems\n","\n","\n","train_df['porter_stemmer'] = train_df['stopwords_removed'].apply(lambda x: porter_stemmer(x))\n","train_df.head()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","      <th>stopwords_removed</th>\n","      <th>porter_stemmer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this earthquake ma...</td>\n","      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n","      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n","      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","      <td>[forest, fire, near, la, rong, sask, canada]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to shelter in place are be...</td>\n","      <td>[all, residents, asked, to, shelter, in, place...</td>\n","      <td>[residents, asked, shelter, place, notified, o...</td>\n","      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13000 people receive wildfires evacuation orde...</td>\n","      <td>[13000, people, receive, wildfires, evacuation...</td>\n","      <td>[13000, people, receive, wildfires, evacuation...</td>\n","      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby alaska as s...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n","      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n","      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                     porter_stemmer\n","0   1  ...  [deed, reason, earthquak, may, allah, forgiv, us]\n","1   4  ...       [forest, fire, near, la, rong, sask, canada]\n","2   5  ...  [resid, ask, shelter, place, notifi, offic, ev...\n","3   6  ...  [13000, peopl, receiv, wildfir, evacu, order, ...\n","4   7  ...  [got, sent, photo, rubi, alaska, smoke, wildfi...\n","\n","[5 rows x 9 columns]"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"0sWsmxYLf269"},"source":["<a id=\"Lemmatization\"></a>\n","\n","## Lemmatization:\n","According to the [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf) book:\n","> Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us ﬁnd all mentions of words in Russian like Moscow. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story.\n","\n","and the book [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action):\n","> Some lemmatizers use the word’s part of speech (POS) tag in addition to its spelling to help improve accuracy. The POS tag for a word indicates its role in the grammar of a phrase or sentence. For example, the noun POS is for words that refer to “people, places, or things” within a phrase. An adjective POS is for a word that modifies or describes a noun. A verb refers to an action. The POS of a word in isolation cannot be determined. The context of a word must be known for its POS to be identified. So some advanced lemmatizers can’t be run-on words in isolation.\n","\n","For example, the \"good\", \"better\" or \"best\" is lemmatized into good and the verb \"gardening\" should be lemmatized to \"to garden\", while the \"garden\" and \"gardener\" are both different lemmas. In this notebook, we will also explore on both lemmatize on without POS-Tagging and POS-Tagging examples.\n"]},{"cell_type":"markdown","metadata":{"id":"G_ZMznh1f26-"},"source":["<a id=\"Lemmatization\"></a>\n","\n","### Lemmatization:\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:48.176982Z","iopub.status.busy":"2021-11-13T10:20:48.176700Z","iopub.status.idle":"2021-11-13T10:20:48.785702Z","shell.execute_reply":"2021-11-13T10:20:48.784673Z","shell.execute_reply.started":"2021-11-13T10:20:48.176953Z"},"id":"346rwkJGf26_","colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"status":"ok","timestamp":1637605401485,"user_tz":-330,"elapsed":2989,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"1256a589-ec84-447a-f59e-50e303c9bde5"},"source":["import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer()\n","#Apply lemmatization on train_df['stopwords_removed'] and save it in train_df['lemmatize_word']\n","\n","train_df['lemmatize_word'] = train_df['stopwords_removed'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n","\n","# Next, remove any newly formed stop words in train_df['lemmatize_word']\n","\n","train_df['lemmatize_word'] = train_df['lemmatize_word'].apply(lambda x: [word for word in x if word not in stop])\n","\n","\n","\n","#Joining the lemmatized tokens back to text\n","train_df['lemmatize_text'] = [' '.join(map(str, l)) for l in train_df['lemmatize_word']] \n","\n","train_df.head()"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>keyword</th>\n","      <th>location</th>\n","      <th>text</th>\n","      <th>target</th>\n","      <th>text_clean</th>\n","      <th>tokenized</th>\n","      <th>stopwords_removed</th>\n","      <th>porter_stemmer</th>\n","      <th>lemmatize_word</th>\n","      <th>lemmatize_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Our Deeds are the Reason of this #earthquake M...</td>\n","      <td>1</td>\n","      <td>our deeds are the reason of this earthquake ma...</td>\n","      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n","      <td>[deeds, reason, earthquake, may, allah, forgiv...</td>\n","      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n","      <td>[deed, reason, earthquake, may, allah, forgive...</td>\n","      <td>deed reason earthquake may allah forgive u</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>4</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Forest fire near La Ronge Sask. Canada</td>\n","      <td>1</td>\n","      <td>forest fire near la ronge sask canada</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","      <td>[forest, fire, near, la, rong, sask, canada]</td>\n","      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n","      <td>forest fire near la ronge sask canada</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>All residents asked to 'shelter in place' are ...</td>\n","      <td>1</td>\n","      <td>all residents asked to shelter in place are be...</td>\n","      <td>[all, residents, asked, to, shelter, in, place...</td>\n","      <td>[residents, asked, shelter, place, notified, o...</td>\n","      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n","      <td>[resident, asked, shelter, place, notified, of...</td>\n","      <td>resident asked shelter place notified officer ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>13,000 people receive #wildfires evacuation or...</td>\n","      <td>1</td>\n","      <td>13000 people receive wildfires evacuation orde...</td>\n","      <td>[13000, people, receive, wildfires, evacuation...</td>\n","      <td>[13000, people, receive, wildfires, evacuation...</td>\n","      <td>[13000, peopl, receiv, wildfir, evacu, order, ...</td>\n","      <td>[13000, people, receive, wildfire, evacuation,...</td>\n","      <td>13000 people receive wildfire evacuation order...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n","      <td>1</td>\n","      <td>just got sent this photo from ruby alaska as s...</td>\n","      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n","      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n","      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n","      <td>[got, sent, photo, ruby, alaska, smoke, wildfi...</td>\n","      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id  ...                                     lemmatize_text\n","0   1  ...         deed reason earthquake may allah forgive u\n","1   4  ...              forest fire near la ronge sask canada\n","2   5  ...  resident asked shelter place notified officer ...\n","3   6  ...  13000 people receive wildfire evacuation order...\n","4   7  ...  got sent photo ruby alaska smoke wildfire pour...\n","\n","[5 rows x 11 columns]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:49.466150Z","iopub.status.busy":"2021-11-13T10:20:49.465755Z","iopub.status.idle":"2021-11-13T10:20:49.473793Z","shell.execute_reply":"2021-11-13T10:20:49.472384Z","shell.execute_reply.started":"2021-11-13T10:20:49.466110Z"},"id":"nZQb83QRf26_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605421597,"user_tz":-330,"elapsed":611,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"909328f3-82ea-4971-adc6-24be219df635"},"source":["#comparison between original text and lemmatized text\n","\n","print(train_df[\"text\"][8])\n","print(train_df[\"lemmatize_word\"][8])"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["There's an emergency evacuation happening now in the building across the street\n","['emergency', 'evacuation', 'happening', 'building', 'across', 'street']\n"]}]},{"cell_type":"markdown","metadata":{"id":"lnPfvl70f27A"},"source":["Comparison between original text and the lammatized text:"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:49.475416Z","iopub.status.busy":"2021-11-13T10:20:49.475133Z","iopub.status.idle":"2021-11-13T10:20:49.508126Z","shell.execute_reply":"2021-11-13T10:20:49.507045Z","shell.execute_reply.started":"2021-11-13T10:20:49.475386Z"},"id":"7AmbhjP7f27A","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1637605427318,"user_tz":-330,"elapsed":502,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"e1868bd9-e88e-4510-f8ae-0eed0aaf0469"},"source":["display(train_df[\"text\"][0], train_df[\"lemmatize_text\"][0])\n","display(train_df[\"text\"][5], train_df[\"lemmatize_text\"][5])\n","display(train_df[\"text\"][10], train_df[\"lemmatize_text\"][10])\n","display(train_df[\"text\"][15], train_df[\"lemmatize_text\"][15])\n","display(train_df[\"text\"][20], train_df[\"lemmatize_text\"][20])"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'deed reason earthquake may allah forgive u'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'rockyfire update california hwy 20 closed direction due lake county fire cafire wildfire'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Three people died from the heat wave so far'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'three people died heat wave far'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"What's up man?\""]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'whats man'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'this is ridiculous....'"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'ridiculous'"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"GBntdaY2f27B"},"source":["# Text Features Extraction:\n","\n","## Weighted Words - Bag of Words (BoW) - Bag of n-grams:\n","**Watch this video: https://www.youtube.com/watch?v=IKgBLTeQQL8**\n","\n","* N-gram is a sequence that contains n-elements (characters, words, etc). A single word such a \"apple\", \"orange\" is a Uni-gram; hence, \"red apple\" \"big orange\" is bi-gram and \"red ripped apple\", \"big orange bag\" is tri-gram. \n","* Bags of words: Vectors of word counts or frequencies \n","* Bags of n-grams: Counts of word pairs (bigrams), triplets (trigrams), and so on\n","\n","> The bag-of-words/ bag-of-n-gram model is a reduced and simpliﬁed representation of a text document from selected parts of the text, based on speciﬁc criteria, such as word frequency.\n","> \n","> In a BoW, a body of text, such as a document or a sentence, is thought of like a bag of words. Lists of words are created in the BoW process. These words in a matrix are not sentences which structure sentences and grammar, and the semantic relationship between these words are ignored in their collection and construction. The words are often representative of the content of a sentence. While grammar and order of appearance are ignored, multiplicity is counted and may be used later to determine the focus points of the documents.\n","> \n","> Example:\n","> Document\n","> \n","> “As the home to UVA’s recognized undergraduate and graduate degree programs in systems engineering. In the UVA Department of Systems and Information Engineering, our students are exposed to a wide range of range”\n","> \n","> Bag-of-Words (BoW):\n","> {“As”, “the”, “home”, “to”, “UVA’s”, “recognized”, “undergraduate”, “and”, “graduate”, “degree”, “program”, “in”, “systems”, “engineering”, “in”, “Department”, “Information”,“students”, “ ”,“are”, “exposed”, “wide”, “range” }\n","> \n","> Bag-of-Feature (BoF)\n","> Feature = {1,1,1,3,2,1,2,1,2,3,1,1,1,2,1,1,1,1,1,1}\n","\n","### Frequency Vectors - CountVectorizer:\n","We will implement the Bag of Words/ Bag of n-grams text representation via sklearn - CountVectorizer function.\n","The code will test with a sample corpus of the first five sentence of the dataset, then print out the output of uni-gram, bi-gram and tri-gram. Finaly, we also run on the whole dataset.\n","\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:49.510031Z","iopub.status.busy":"2021-11-13T10:20:49.509756Z","iopub.status.idle":"2021-11-13T10:20:49.517122Z","shell.execute_reply":"2021-11-13T10:20:49.516090Z","shell.execute_reply.started":"2021-11-13T10:20:49.510003Z"},"scrolled":true,"id":"3gV0TI6mf27C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605467020,"user_tz":-330,"elapsed":8388,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"5699bca0-0c93-464a-a39b-8edc57dc4d77"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# implement CountVectorizer for 1-gram, 2-gram and 3-gram on train_df[\"lemmatize_text\"].tolist()\n","\n","def cv(data, ngram = 1, MAX_NB_WORDS = 75000):\n","    \"\"\"\n","        return transformed_dataset, CountVectorizer_object\n","    \"\"\"\n","    count_vectorizer = CountVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)\n","    emb = count_vectorizer.fit_transform(data).toarray()\n","    print(\"count vectorize with\", str(np.array(emb).shape[1]), \"features\")\n","    return emb, count_vectorizer\n","    \n","\n","\n","# applying 1-gram, 2-gram and 3-gram into the whole dataset\n","\n","train_df_corpus = train_df[\"lemmatize_text\"].tolist()\n","train_df_em_1gram, cv_1gram = cv(train_df_corpus, ngram=1)\n","train_df_em_2gram, cv_2gram = cv(train_df_corpus, ngram=2)\n","train_df_em_3gram, cv_3gram = cv(train_df_corpus, ngram=3)\n","\n","print(len(train_df_corpus))\n","print(train_df_em_1gram.shape)\n","print(train_df_em_2gram.shape)\n","print(train_df_em_3gram.shape)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["count vectorize with 16296 features\n","count vectorize with 47134 features\n","count vectorize with 45043 features\n","7613\n","(7613, 16296)\n","(7613, 47134)\n","(7613, 45043)\n"]}]},{"cell_type":"markdown","metadata":{"id":"K0xLIrfDf27C"},"source":["<a id=\"TF_IDF\"></a>\n","\n","### Term Frequency-Inverse Document Frequency (TF-IDF):\n","**Watch this video: https://www.youtube.com/watch?v=D2V1okCEsiE**\n","> The Inverse Document Frequency (IDF) as a method to be used in conjunction with term frequency in order to lessen the effect of implicitly common words in the corpus. IDF assigns a higher weight to words with either high or low frequencies term in the document. This combination of TF and IDF is well known as Term Frequency-Inverse document frequency (TF-IDF). The mathematical representation of the weight of a term in a document by TF-IDF is given in Equation: \n","> $$ W(d,t) = TF(d,t) * log \\frac{N}{df(t)}$$\n","> Here N is the number of documents and $df(t)$ is the number of documents containing the term t in the corpus. The ﬁrst term in the equation improves the recall while the second term improves the precision of the word embedding. Although TF-IDF tries to overcome the problem of common terms in the document, it still suffers from some other descriptive limitations. Namely, TF-IDF cannot account for the similarity between the words in the document since each word is independently presented as an index. However, with the development of more complex models in recent years, new methods, such as word embedding, have been presented that can incorporate concepts such as similarity of words and part of speech tagging.\n","\n","(source: [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n","\n","We also implement the TF-IDF via sklearn TfidfVectorizer function, the experiments are similar to the previous section\n","\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:52.328871Z","iopub.status.busy":"2021-11-13T10:20:52.328590Z","iopub.status.idle":"2021-11-13T10:20:52.345690Z","shell.execute_reply":"2021-11-13T10:20:52.343526Z","shell.execute_reply.started":"2021-11-13T10:20:52.328842Z"},"scrolled":true,"id":"SpZZeC9vf27D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605486575,"user_tz":-330,"elapsed":1533,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"ddce573e-3c9c-448f-e7cc-71f3720a32f1"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","#implement tfid vectorizer for ngram=1, on train_df[\"lemmatize_text\"].tolist() and print its shape\n","def TFIDF(data, ngram = 1, MAX_NB_WORDS = 75000):\n","    \"\"\"\n","        return transformed_dataset, TfidfVectorizer_object\n","    \"\"\"\n","    tfidf_x = TfidfVectorizer(ngram_range = (ngram, ngram), max_features = MAX_NB_WORDS)\n","    emb = tfidf_x.fit_transform(data).toarray()\n","    print(\"tf-idf with\", str(np.array(emb).shape[1]), \"features\")\n","    return emb, tfidf_x\n","\n","train_df_corpus = train_df[\"lemmatize_text\"].tolist()\n","train_df_tfidf_1gram, tfidf_1gram = TFIDF(train_df_corpus, 1)\n","\n","print(len(train_df_corpus))\n","print(train_df_tfidf_1gram.shape)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tf-idf with 16296 features\n","7613\n","(7613, 16296)\n"]}]},{"cell_type":"markdown","metadata":{"id":"23ohwQDNf27E"},"source":["<a id=\"Word_Embedding\"></a>\n","\n","## Word Embedding:\n","\n","> **Word vectors** are numerical vector representations of word semantics, or meaning, including literal and implied meaning. So word vectors can capture the connotation of words, like “peopleness,” “animalness,” “placeness,” “thingness,” and even “conceptness.” And they combine all that into a dense vector (no zeros) of floating point values. This dense vector enables queries and logical reasoning.\n","\n","(source: [Natural Language Processing in Action](https://www.manning.com/books/natural-language-processing-in-action))\n","\n","> Even though we have syntactic word representations, it does not mean that the model captures the semantics meaning of the words. On the other hand, bag-of-word models do not respect the semantics of the word. For example, words “airplane”, “aeroplane”, “plane”, and “aircraft” are often used in the same context. However, the vectors corresponding to these words are orthogonal in the bag-of-words model. This issue presents a serious problem to understanding sentences within the model. The other problem in the bag-of-word is that the order of words in the phrase is not respected. The n-gram does not solve this problem so a similarity needs to be found for each word in the sentence. Many researchers worked on word embedding to solve this problem. The Word2Vec propose a simple single-layer architecture based on the inner product between two word vectors.\n","\n","> Word embedding is a feature learning technique in which each word or phrase from the vocabulary is mapped to a N dimension vector of real numbers. Various word embedding methods have been proposed to translate unigrams into understandable input for machine learning algorithms. This work focuses on Word2Vec, GloVe, and FastText, three of the most common methods that have been successfully used for deep learning techniques.\n","\n","(source: [Text Classification Algorithms: A Survey](https://arxiv.org/abs/1904.08067))\n","\n","<a id=\"Basic_Word_Embedding\"></a>\n","### Basic Word Embedding Methods:\n","\n","**Watch this video: https://www.youtube.com/watch?v=pO_6Jk0QtKw**\n","#### Word2Vec:\n","\n","[T. Mikolov et al.](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) presented the Word2vec in 2013, which learns the meaning of words merely by processing a large corpus of unlabeled text. The Word2Vec approach uses shallow neural networks with two hidden layers, continuous bag-of-words (CBOW), and the Skip-gram model to create a high dimension vector for each word. This unsupervised nature of Word2vec is what makes it so powerful. The world is full of unlabeled, uncategorized, unstructured natural language text.\n","\n","We will implement the Word2vec via gensim libary \n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:54.904469Z","iopub.status.busy":"2021-11-13T10:20:54.904142Z","iopub.status.idle":"2021-11-13T10:20:57.389866Z","shell.execute_reply":"2021-11-13T10:20:57.389040Z","shell.execute_reply.started":"2021-11-13T10:20:54.904438Z"},"scrolled":true,"id":"RAXBQN8Yf27F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605511530,"user_tz":-330,"elapsed":1406,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"22febdc6-9916-4f07-ce65-acee79e59a94"},"source":["import gensim\n","document = train_df[\"lemmatize_word\"].tolist()\n","# train the Word2vec model on train_df[\"lemmatize_word\"].tolist()\n","model = gensim.models.Word2Vec(document, min_count=10, window=5, size=100)\n","# print summary of the model\n","print(model)\n","# print vector for any one word\n","model.wv['forest']"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Word2Vec(vocab=1427, size=100, alpha=0.025)\n"]},{"output_type":"execute_result","data":{"text/plain":["array([-0.1969117 , -0.2607379 , -0.3440599 , -0.13323446,  0.02887042,\n","        0.34812656,  0.14446816,  0.02145006,  0.00441775,  0.06279428,\n","        0.25556156, -0.02799619,  0.19534054,  0.16541621, -0.5075848 ,\n","       -0.10426405,  0.08021374,  0.4392956 , -0.06420341, -0.06826293,\n","       -0.11051098,  0.38958955, -0.10715923,  0.4141505 ,  0.00115766,\n","       -0.04856926, -0.01033854,  0.15541309, -0.14110103, -0.00484465,\n","       -0.05717436,  0.21171452, -0.06272054,  0.0033036 ,  0.22910334,\n","        0.40617177,  0.08168331,  0.22631137,  0.27861574,  0.24004732,\n","       -0.076285  ,  0.36759683,  0.02500217, -0.05804264,  0.35939646,\n","        0.35119548,  0.1352884 , -0.03678621, -0.06787793, -0.20319717,\n","        0.07438607, -0.29399782,  0.20588747,  0.3913207 ,  0.3327444 ,\n","        0.5134895 , -0.45595303, -0.06270483,  0.03618896,  0.41516992,\n","        0.21023177,  0.02778741,  0.33345017,  0.12453092,  0.24397573,\n","       -0.08374276, -0.36232325,  0.19449282, -0.10526721,  0.47530982,\n","       -0.1178041 , -0.13357307,  0.00979321, -0.4428036 ,  0.37412584,\n","        0.19138066,  0.28100657, -0.10883807, -0.02141133, -0.1078485 ,\n","        0.09001319, -0.14421913, -0.20426966, -0.05971538,  0.01831023,\n","        0.01408254,  0.41928318, -0.06512402, -0.26421085,  0.33826765,\n","        0.22756426,  0.5651858 ,  0.01896651, -0.26962972,  0.00766847,\n","       -0.01792269, -0.01018231, -0.0120463 ,  0.36881953,  0.11707921],\n","      dtype=float32)"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:57.391319Z","iopub.status.busy":"2021-11-13T10:20:57.391049Z","iopub.status.idle":"2021-11-13T10:20:57.396403Z","shell.execute_reply":"2021-11-13T10:20:57.395837Z","shell.execute_reply.started":"2021-11-13T10:20:57.391291Z"},"id":"pauJvU2Df27F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637605525319,"user_tz":-330,"elapsed":549,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"a4c78882-5d9d-4800-9729-baa7aa457045"},"source":["#print the similarity between any two words in the dataset\n","print(model.similarity('wildfire', 'fire'))\n","print(model.similarity('people', 'officer'))"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9996301\n","0.9996937\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}]},{"cell_type":"markdown","metadata":{"id":"f85LBuGgf27F"},"source":["<a id=\"GloVe\"></a>\n","\n","#### Global Vectors for Word Representation (GloVe):\n","> Another powerful word embedding technique that has been used for text classiﬁcation is [Global Vectors (GloVe)](https://nlp.stanford.edu/pubs/glove.pdf). The approach is very similar to the Word2Vec method, where each word is presented by a high dimension vector and trained based on the surrounding words over a huge corpus. The pre-trained word embedding used in many works is based on 400,000 vocabularies trained over Wikipedia 2014 and Gigaword 5 as the corpus and 50 dimensions for word presentation. GloVe also provides other pre-trained word vectorizations with 100, 200, 300 dimensions which are trained over even bigger corpora, including Twitter content.\n","\n","We will create our GloVe's sentence embeddings  via gensim libary with the pre-trained word vectors on the dataset from Wikipedia 2014 + Gigaword 5 (source: https://github.com/stanfordnlp/GloVe) and see the embedding output on the sample sentence from the our dataset. \n","\n","\n"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:20:58.560808Z","iopub.status.busy":"2021-11-13T10:20:58.560428Z","iopub.status.idle":"2021-11-13T10:22:43.013605Z","shell.execute_reply":"2021-11-13T10:22:43.012543Z","shell.execute_reply.started":"2021-11-13T10:20:58.560780Z"},"scrolled":true,"id":"ixkTLtAxf27G","colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"status":"error","timestamp":1637605715149,"user_tz":-330,"elapsed":728,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"9ef56233-3575-4440-b9e1-46b79743b79b"},"source":["from gensim.scripts.glove2word2vec import glove2word2vec\n","import gensim\n","#download glove embeddings from: https://www.kaggle.com/anindya2906/glove6b and save it in the same folder as this task\n","\n","glove_input_file = \"/content/drive/MyDrive/glove.6B.100d.txt\" #if you have named the file differently, then make changes here\n","word2vec_output_file = \"glove.6B.100d.txt.word2vec\"\n","glove2word2vec(glove_input_file, word2vec_output_file) #converting glove_input_file in word format to word2vec format\n","glove_model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False, limit=200000) "],"execution_count":26,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-99d9f874dd8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mglove_input_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/glove.6B.100d.txt\"\u001b[0m \u001b[0;31m#if you have named the file differently, then make changes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mword2vec_output_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove.6B.100d.txt.word2vec\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#converting glove_input_file in word format to word2vec format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mglove_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mglove2word2vec\u001b[0;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \"\"\"\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_glove_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"converting %i vectors from %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_input_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2vec_output_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/scripts/glove2word2vec.py\u001b[0m in \u001b[0;36mget_glove_info\u001b[0;34m(glove_file_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mnum_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/glove.6B.100d.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"nH96LDGNf27H"},"source":["Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\" from GloVe"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-11-13T10:22:43.014821Z","iopub.status.busy":"2021-11-13T10:22:43.014607Z","iopub.status.idle":"2021-11-13T10:22:43.022062Z","shell.execute_reply":"2021-11-13T10:22:43.021216Z","shell.execute_reply.started":"2021-11-13T10:22:43.014799Z"},"id":"g53XM-Eaf27H","colab":{"base_uri":"https://localhost:8080/","height":200},"executionInfo":{"status":"error","timestamp":1637605586119,"user_tz":-330,"elapsed":524,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"69151fce-d622-481f-ae0e-40b598679650"},"source":["# Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\" from the GloVe Model\n","print(glove_model.similarity('cat', 'kitten'))\n","print(glove_model.similarity('cat', 'cats'))"],"execution_count":23,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-674f9a0d1e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Compare the similarity between \"cat\" vs. \"kitten\" and \"cat\" vs. \"cats\" from the GloVe Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kitten'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cats'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'glove_model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"RlVuWk3bf27I"},"source":["## T-SNE"]},{"cell_type":"markdown","metadata":{"id":"2tG9ASrHf27I"},"source":["**Watch This Video: https://www.youtube.com/watch?v=NEaUSP4YerM**<br>\n","Now, lets visualize some of our embeddings. To plot embeddings with a dimensionality of 100 or more, we first need to map them to a dimensionality of 2. We do this with the popular t-SNE method. T-SNE, short for t-distributed Stochastic Neighbor Embedding, helps us visualize high-dimensional data by mapping similar data to nearby points and dissimilar data to distance points in the low-dimensional space.\n","\n","T-SNE is present in Scikit-learn. To run it, we just have to specify the number of dimensions we'd like to map the data to (n_components), and the similarity metric that t-SNE should use to compute the similarity between two data points (metric). We're going to map to 2 dimensions and use the cosine as our similarity metric. Additionally, we use PCA as an initialization method to remove some noise and speed up computation. The Scikit-learn user guide contains some additional tips for optimizing performance.\n","\n","Plotting all the embeddings in our vector space would result in a very crowded figure where the labels are hardly legible. Therefore we'll focus on a subset of embeddings by selecting the 200 most similar words to a target word.\n","\n"]},{"cell_type":"code","metadata":{"id":"TxP7SqGLf27I","colab":{"base_uri":"https://localhost:8080/","height":234},"executionInfo":{"status":"error","timestamp":1637605657190,"user_tz":-330,"elapsed":628,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"4185243f-988d-415b-ba1a-162127ba5c04"},"source":["#tSNE\n","import pandas as pd\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","from sklearn.manifold import TSNE\n","\n","target_word = \"body\"\n","selected_words = [w[0] for w in glove_model.wv.most_similar(positive=[target_word], topn=200)] + [target_word]\n","embeddings = [glove_model.wv[w] for w in selected_words] + glove_model.wv[\"body\"]\n","\n","mapped_embeddings = TSNE(n_components=2, metric='cosine', init='pca').fit_transform(embeddings)"],"execution_count":24,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-b27fdcd014bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtarget_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"body\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mselected_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtarget_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_words\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mglove_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"body\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'glove_model' is not defined"]}]},{"cell_type":"code","metadata":{"id":"HHfM9Vxvf27J","colab":{"base_uri":"https://localhost:8080/","height":252},"executionInfo":{"status":"error","timestamp":1637605672784,"user_tz":-330,"elapsed":554,"user":{"displayName":"aditya shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00437549320141341008"}},"outputId":"855acbac-cf46-47fc-c1de-8585f7cf3b6c"},"source":["plt.figure(figsize=(20,20))\n","x = mapped_embeddings[:,0]\n","y = mapped_embeddings[:,1]\n","plt.scatter(x, y)\n","\n","for i, txt in enumerate(selected_words):\n","    plt.annotate(txt, (x[i], y[i]))"],"execution_count":25,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-bfc039f9243d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapped_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmapped_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'mapped_embeddings' is not defined"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1440x1440 with 0 Axes>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"G86wbo7g54K7"},"source":[""],"execution_count":null,"outputs":[]}]}